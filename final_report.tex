\documentclass[11pt,letterpaper,twocolumn]{article}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{cite}
\usepackage{amsmath}
\usepackage{enumitem}
\usepackage{tabularx,array,booktabs}
\usepackage{multirow}
\usepackage{float}
\usepackage{stfloats}
\usepackage{placeins}

\title{\textbf{Hedonic Forecast: ML Housing Prediction in Japan}}
\author{Team 90: Jianhuang Li, Shin Ying Chua, Wei Qi Thong, Ryan Kai Yan Seet}
\date{}

\begin{document}

\maketitle

\section{Introduction}
Urban buyers, developers, and planners in Japan wait roughly three months for the Japan Residential Property Price Index (JRPPI), which also reports on broad geographic regions\cite{jrppi2020,jrppi_timelag}. To close that gap, we build \emph{Hedonic Forecast}, a reproducible pipeline that converts 485,093 Ministry of Land, Infrastructure, Transport and Tourism (MLIT) transactions\cite{mlit_data} into ward- and 250\,m mesh forecasts for Tokyo's 23 wards and Sendai.

The pipeline combines analytics modelling with a Streamlit dashboard: choropleths, leaderboards, and SHapley Additive exPlanations (SHAP) give stakeholders both the numbers and the reasons behind them. While the figures in this paper are static, every figure is generated from the same reproducible code. The system consists of four main components: (1) quarterly mesh hedonic indices that keep rows with missing building years via indicators, (2) hierarchical forecasting where mesh models ingest ward medians/lags to stabilize sparse geographies, (3) direct mapping from MLIT coordinates to JIS \emph{X~0410} meshes without external geocoding\cite{stats_mesh,jshis_250m}, and (4) cold-start smoothing that follows state-space ETS (Error, Trend, Seasonality) initialization so early-quarter predictions do not spike\cite{hyndman2002ets}. These help keep the experience interpretable while delivering ward-level MAE scores of 18,595~JPY/m$^2$.

\section{Problem Definition}

We address three main tasks: (1) construct quarterly hedonic price indices at ward/mesh levels summarizing spatiotemporal dynamics from 2005--2025; (2) predict next-quarter median JPY/m$^2$ using only data available at prediction time; (3) provide interpretable  visualizations of spatial patterns . 

The model inputs utilized are MLIT real-estate transaction features (price, area, date, building year, location)\cite{mlit_data}. Outputs include the computed pricing indices, model-generated one-step forward predictions with uncertainty values, and standard metrics (MAE, RMSE, $R^2$), accompanied by SHAP explanations\cite{hyndman2006,tashman2000,shap2017}.

Formally, we let $p_{l,t}$ denote the median price-per-square-meter for location $l$ at quarter $t$ and $x_{l,t}$ the engineered feature vector, we learn $f_l$ such that $\hat{p}_{l,t+1}=f_l(x_{l,t})$ by minimizing empirical mean squared error (MSE) subject to chronological splits (Train 2009--2019 Q4, Val 2020 Q1--2021 Q4, Test 2022 Q1--2025 Q1). We additionally require $f_l$ to supply additive feature attributions (via SHAP).

\section{Literature Survey}
Japanese real estate forecasting has multiple problems. Official JRPPI indices \cite{jrppi2020} lag 2-3 months, rendering them less useful for investors and stakeholders who need timely signals. Academic models are generally split between spatial and temporal factor models: spatial ML \cite{yoshida2024} ignores time, while temporal models such as LSTMs \cite{lstm2017} ignore location. Transformers have the potential to handle both but have high computational costs and ultimately prove difficult to interpret\cite{transformers_survey}.

Research also shows that nearby transactions influence local prices \cite{spatiotemporal2023}, but modeling full spatial covariance structures is computationally intensive. We use mesh aggregation \cite{jsai2022}; a grid-based shortcut that captures neighborhood effects efficiently. For interpretability, we apply SHAP \cite{shap2017, lundberg2020nmi}, which quantifies the importance of characteristics in models including LSTMs \cite{shap_lstm2025}. Some articles have also proven that multivariate LSTMs that incorporate multiple input features consistently outperform univariate approaches in financial and economic forecasting \cite{multivariate_lstm2022}. Operational research on Tokyo's 23 wards also demonstrates practical MLIT data cleaning procedures and adaptive rolling windows \cite{hitotsubashi2023} which we will utilize in our approach to work with the dataset. Peng and Inoue \cite{peng2022} apply eigenvector spatial filtering to show that Tokyo housing prices reflect both local factors, such as nearby schools or train stations, and regional effects across wards. This highlights the need for multi-scale spatial features in our forecasting models, though their study is mainly explanatory rather than predictive.

Overall, this review reveals multiple gaps in existing literature. Official pricing indices have delays, spatial-temporal effects are often separated, models lack transparency, and contemporary research focuses overwhelmingly on Tokyo. No work systematically compares multiple ML approaches with SHAP interpretability and interactive visualization while comparing Tokyo with regional cities.

\section{Proposed Method}

\subsection{Data and Preprocessing}
We cleaned a total of 485,093 MLIT transactions (2005-Q3 to 2025-Q1) across Tokyo's 23 wards and Sendai's main wards, covering $\sim$3,000+ unique 250\,m meshes. Data cleaning addressed full-width numerals, Japanese era calendars, and missing building years (median-imputed by ward with \texttt{AgeUnknown} indicator). Spatial encoding derives \emph{JIS X~0410} 250\,m mesh codes\cite{stats_mesh} directly from MLIT transaction coordinates (lat/lon), then collapses to district-quarter medians. A ward-centroid fallback is then applied if coordinates are missing. This avoids separate geocoding and reduces processing overheads substantially\cite{mlit_data,stats_mesh,jshis_250m}. Two panels are created as a result: ward$\times$quarter (28 wards) and mesh$\times$quarter, each with a set of median/dispersion price metrics, transaction counts, building statistics, and temporal keys.

\noindent The MLIT endpoint mixes structured and semi-structured values, so we apply normalization on numerical data, harmonize Japanese era dates to Gregorian quarters, and standardize areas to square meters. Transactions with blank coordinates trigger a ward-level centroid fallback so every row inherits a mesh code, while a secondary fallback copies the most recent valid mesh code when the MLIT dataset suppresses coordinates for privacy. These steps keep the spatial grid sufficiently dense even when lat/lon values are missing.

\begin{table}[H]
\centering
\caption{Dataset summary and temporal splits}
\label{tab:data_summary}
\begin{tabular}{@{}lr@{}}
\toprule
\textbf{Metric} & \textbf{Value} \\
\midrule
Total transactions & 485,093 \\
Time horizon & 2005-Q3 to 2025-Q1 \\
Unique 250m meshes & $\sim$3,000+ \\
Train/Val/Test (Ward) & 1,507 / 336 / 252 \\
Train/Val/Test (Mesh) & 26,818 / 6,337 / 4,762 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Intuition}
Recent real-estate forecasting efforts in Japan tend to emphasize a single model family. Yoshida and Seya~\cite{yoshida2024} benchmark linear regression against gradient boosted trees using macro/static predictors, while Otsuki~\cite{hitotsubashi2023} focuses on data-cleaning heuristics without combining interpretable and neural approaches. Existing LSTM work on housing~\cite{lstm2017} largely treats the task as univariate sequence prediction, and most studies lack feature-level explanations to justify deployment~\cite{shap2017, lundberg2020nmi}.

Our approach attempts to blend complementary ideas so the user can trace why each choice improves stability or transparency:
\begin{itemize}[leftmargin=*]
    \item \textbf{Hierarchical hedonic context.} We reconstruct ward and mesh hedonic indices quarterly, propagating ward-level fixed effects down to meshes via explicit missingness indicators. This retains the stability of municipality-level priors\cite{hitotsubashi2023} while preserving the high spatial resolution required by decision-makers.
    \item \textbf{Model diversity.} By training linear, tree-based, and LSTM sequence models in parallel, we isolate specific data characteristics: linear models capture broad trends, trees identify non-linear price regimes (e.g., the ``Sendai Penalty''), and LSTMs test the limits of pure sequence learning.
    \item \textbf{Built-in interpretability.} We integrate SHAP attribution directly into the pipeline. This addresses the ``black box'' limitations of neural approaches\cite{shap_lstm2025}, allowing analysts to verify whether forecasts are driven by fundamental momentum or structural anomalies.
\end{itemize}
    
We keep the feature sets compact by design. Fifteen ward features capture price momentum (lags, moving averages), liquidity (transaction counts), and structural signals (age, area) without overfitting the $\sim$1.7k ward samples, while the 13+ mesh features add hedonic indices and missingness flags to stabilize sparse grids. This balance is enough to outperform naive lag/MA baselines while preserving interpretability.

\subsection{Reproducibility and Workflow}
All figures and tables come from the CLI pipeline in \texttt{CODE/}: \texttt{python -m CODE.run\_workflow} rebuilds panels, trains models with the stated splits, exports SHAP, and writes outputs to \texttt{CODE/outputs/}. Efficiency tables use \texttt{python -m CODE.experiments --levels Ward Mesh --fractions 0.25 0.5 0.75 1.0}. Datasets are scraped and processed from the publicly available MLIT API\cite{mlit_data}.

\subsection{Interactive Dashboard}
The Streamlit dashboard exposes the artifacts from our analyses through three panes: (1) a mesh-250\,m choropleth with quarter auto-play, city/model selectors, and metric toggles (price index, median/mean JPY/m$^2$, transaction counts, age, area); (2) a leaderboard sourced from our model results that highlights the top model and surfaces MAE/RMSE/$R^2$; and (3) SHAP explainability that pairs global bar/beeswarm plots with local force/waterfall views. Figure~\ref{fig:dashboard} illustrates the choropleth and sidebar controls that drive these panels.

\begin{figure}[H]
\centering
\IfFileExists{dashboard.png}{%
    \includegraphics[width=\linewidth]{dashboard.png}%
}{%
    \fbox{\parbox{0.9\linewidth}{\centering Dashboard screenshot placeholder (mesh map, leaderboard, SHAP panels).\linebreak Add \texttt{visualisation/dashboard.png} to replace.}}%
}
\caption{Streamlit dashboard --- using mesh-250\,m data, choropleth with sidebar controls, leaderboard, and SHAP panels.}
\label{fig:dashboard}
\end{figure}
\subsection{Detailed Description}
\textbf{Hedonic price index.} We estimate municipality-level indices using two-way fixed effects (PanelOLS)\cite{c}:
\begin{equation}
\label{eq:hedonic}
\small
\begin{aligned}
\ln(P_{it}) =\; & \beta_0 + \beta_1 \ln(\text{Area}_{it}) + \beta_2 \text{Age}_{it} \\
& {}+ \beta_3 \text{AgeUnknown}_{it} + \alpha_i + \gamma_t + \varepsilon_{it}
\end{aligned}
\end{equation}
where $\alpha_i$ (mesh fixed effects) and $\gamma_t$ (quarter fixed effects) capture spatial and temporal heterogeneity. The \texttt{AgeUnknown} dummy handles missing building year values without dropping observations. Quarterly granularity exceeds typical annual approaches. Fitted log prices are averaged by mesh-quarter, exponentiated, and normalized based on existing formulas\cite{c} to construct 2005--2025 indices. We merge these tables into the ward/mesh panels, forward/back-fill missing meshes with ward-level scores, and record binary missing indicators so validation/test rows remain populated.

\textbf{Forecasting models.} We train separate ward and mesh models with predefined chronological splits (Train $\le$ 2019-Q4, Val 2020-Q1--2021-Q4, Test $\ge$ 2022-Q1). Feature sets utilized for the models are as follows:
\begin{table}[H]
\centering
\scriptsize
\caption{Ward feature set (15 columns).}
\label{tab:ward-features}
\begin{tabular}{@{}lp{5.5cm}@{}}
\toprule
Category & Features \\
\midrule
Momentum & \texttt{MedianPriceSqM\_lag1}, \texttt{MedianPriceSqM\_lag4}, QoQ/YoY growth, \texttt{MedianPriceSqM\_ma4q}, \texttt{MedianPriceSqM\_std4q} \\
Liquidity & \texttt{TransactionCount}, \texttt{ActiveMeshes} \\
Structure & \texttt{AvgBuildingAge}, \texttt{AvgArea} \\
Time/ID & \texttt{TimeTrend}, \texttt{Ward\_en\-coded}, \texttt{Q\_2}, \texttt{Q\_3}, \texttt{Q\_4} \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\scriptsize
\caption{Mesh feature set (13{+} columns).}
\label{tab:mesh-features}
\begin{tabular}{@{}lp{5.5cm}@{}}
\toprule
Category & Features \\
\midrule
Momentum & \texttt{mesh\_median\_ppsqm\_lag1}, \texttt{mesh\_median\_ppsqm\_lag4}, QoQ/YoY growth, \texttt{mesh\_median\_ppsqm\_ma4q}, \texttt{mesh\_median\_ppsqm\_std4q} \\
Liquidity & \texttt{mesh\_trans\-action\_count}, \texttt{mesh\_avg\_age}, \texttt{mesh\_avg\_area} \\
Context & \texttt{TimeTrend}, \texttt{Ward\_en\-coded}, \texttt{WardHedonicIndex}, \texttt{MeshHedonicIndex} \\
Missingness & \texttt{WardHedonicIndex\_missing}, \texttt{MeshHedonicIndex\_missing} \\
\bottomrule
\end{tabular}
\end{table}

The LSTM models use reduced feature sets focused on core momentum and liquidity signals:

\begin{table}[H]
\centering
\scriptsize
\caption{Ward LSTM feature set (7 columns).}
\label{tab:ward-lstm-features}
\begin{tabular}{@{}lp{5.5cm}@{}}
\toprule
Category & Features \\
\midrule
Core momentum & \texttt{MedianPriceSqM}, \texttt{MedianPriceSqM\_lag1}, \texttt{MedianPriceSqM\_ma4q}, QoQ growth \\
Liquidity & \texttt{TransactionCount}, \texttt{ActiveMeshes} \\
Structure & \texttt{AvgBuildingAge} \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\scriptsize
\caption{Mesh LSTM feature set (7 columns).}
\label{tab:mesh-lstm-features}
\begin{tabular}{@{}lp{5.5cm}@{}}
\toprule
Category & Features \\
\midrule
Core momentum & \texttt{mesh\_median\_ppsqm}, \texttt{mesh\_median\_ppsqm\_lag1}, \texttt{mesh\_median\_ppsqm\_ma4q}, QoQ growth \\
Liquidity & \texttt{mesh\_trans\-action\_count} \\
Structure & \texttt{mesh\_avg\_age}, \texttt{mesh\_avg\_area} \\
\bottomrule
\end{tabular}
\end{table}

Models use a set of arbitrary SHAP caps in order to avoid long running computations: Random Forests (RF) (300 trees), LightGBM (600 trees), LSTM window 8 quarters with hidden 96, dropout 0.3, and early stopping patience value of 8. Our SHAP process uses the full validation set for metric computation and caps only the explainer sampling to prioritize speed (trees 500 rows, linear models set to 200 rows, and LSTM to 120 sequences, plots to 1,000 rows) so Sendai's small-sample tails stay visible while keeping runtimes manageable. The models themselves train on all available global data.

\section{Evaluation}

\subsubsection*{Testbed Description}
We evaluate on the chronological test split (2022Q1--2025Q1) using the ward and mesh panels described in Section 4.1. Cold-start predictions initialize missing lags with exponential smoothing\cite{hyndman2002ets} to avoid unstable first-quarter forecasts. SHAP values are computed on the validation set, and LSTM timing is excluded from efficiency comparisons due to hardware dependencies.

\subsubsection*{Key Findings}
\begin{enumerate}
    \item \textbf{Feature Dominance and Persistence.} Across all model families, momentum features (moving averages and lags) dominate  (Fig.~\ref{fig:shap-beeswarm}). This is most visible in the Ward Linear Regression ($R^2=0.995$), where feature importance is driven almost exclusively by \texttt{ma4q}. This near-perfect score should be interpreted as a ``persistence baseline'': since ward-level prices are highly smoothed aggregates, tomorrow's price is effectively an extrapolation of today's trend. The lower $R^2$ at the mesh level (0.876) reveals the true challenge of modeling neighborhood-level variance.

    \item \textbf{Regional Asymmetry.} A crucial divergence appears between Tokyo and Sendai. In Tokyo, momentum effects are relatively symmetric. In Sendai (Fig.~\ref{fig:shap-beeswarm}, right), the \texttt{growth\_qoq} feature exhibits a massive negative tail (SHAP values reaching -400,000), while positive contributions are bounded. This suggests the models have learned that in regional markets with lower liquidity, a cooling trend is a severe pricing penalty, whereas heating trends yield diminishing returns. Tokyo's deep liquidity buffers against this asymmetry.

    \item \textbf{LSTM Redundancy.} The LSTM SHAP rankings mirror the Random Forest results almost exactly, with lags and moving averages absorbing the majority of importance. This reveals that for quarterly housing data, the LSTM is primarily functioning as a complex autoregressor. It fails to extract deeper temporal latent states that justify its training overhead, reinforcing the decision to prefer LightGBM or Random Forest for production deployment.

    \item \textbf{Efficiency and Scaling.} We stress-test the classical models by training on 25\%, 50\%, 75\%, and 100\% of historical rows. Appendix Tables~\ref{tab:ward-efficiency} and \ref{tab:mesh-efficiency} summarize the runtime/accuracy trade-offs. Linear Regression offers near-instant inference regardless of training fraction, while Random Forest delivers the best accuracy-time balance once data coverage exceeds 50\%.
\end{enumerate}

\begin{table}[H]
\centering
\scriptsize
\caption{Ward-level test accuracy (JPY/m$^2$).}
\label{tab:ward-accuracy}
\begin{tabular}{lccc}
\toprule
Model & Test MAE & Test RMSE & $R^2$ \\
\midrule
Linear Regression & 18{,}595 & 29{,}651 & 0.995 \\
Random Forest     & 44{,}302 & 112{,}689 & 0.933 \\
LightGBM          & 48{,}039 & 116{,}653 & 0.928 \\
Torch LSTM        & 41{,}101 & 61{,}214  & 0.980 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\scriptsize
\caption{Mesh-level test accuracy (JPY/m$^2$).}
\label{tab:mesh-accuracy}
\begin{tabular}{lccc}
\toprule
Model & Test MAE & Test RMSE & $R^2$ \\
\midrule
Linear Regression & 83{,}896 & 197{,}137 & 0.876 \\
Random Forest     & 22{,}620 & 123{,}618 & 0.951 \\
LightGBM          & 27{,}702 & 126{,}969 & 0.948 \\
Torch LSTM        & 103{,}143 & 324{,}578 & 0.664 \\
\bottomrule
\end{tabular}
\end{table}

Linear models remain strongest for wards because momentum alone explains nearly all the variance. Meshes benefit from nonlinear splits that capture interactions between hedonic indices, lags, and missingness flags; hence Random Forest leads and LightGBM follows closely. The LSTM struggles on meshes because short sequences and limited spatial context make it hard to generalise beyond momentum, and Sendai's small sample triggers LightGBM split warnings, so we prefer Random Forest for Sendai deployment. Ward forecasts can lean on Linear Regression for sub-millisecond updates when latency matters, while meshes warrant Random Forest because its accuracy gains outweigh a few extra milliseconds once training coverage exceeds 50\%.
\FloatBarrier
\noindent Figure~\ref{fig:shap-beeswarm} shows city-split SHAP beeswarms for the tree models (ward LightGBM, mesh Random Forest) plus the available LSTM variants. Mesh LSTM SHAP is provided for Tokyo (validation) and the full test set; a Sendai-specific mesh LSTM plot was not generated.

\begin{figure*}[t]
\centering
\begin{minipage}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{shap_plots/ward_lightgbm_val_tokyo_beeswarm.png}
    \caption*{Tokyo Ward LightGBM}
\end{minipage}\hfill
\begin{minipage}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{shap_plots/ward_lightgbm_val_sendai_beeswarm.png}
    \caption*{Sendai Ward LightGBM}
\end{minipage}\hfill
\begin{minipage}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{shap_plots/mesh_randomforest_val_tokyo_beeswarm.png}
    \caption*{Tokyo Mesh RF}
\end{minipage}\hfill
\begin{minipage}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{shap_plots/mesh_randomforest_val_sendai_beeswarm.png}
    \caption*{Sendai Mesh RF}
\end{minipage}

\vspace{0.5em}
\begin{minipage}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{shap_plots/ward_torchlstm_val_tokyo_beeswarm.png}
    \caption*{Tokyo Ward LSTM}
\end{minipage}\hfill
\begin{minipage}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{shap_plots/ward_torchlstm_val_sendai_beeswarm.png}
    \caption*{Sendai Ward LSTM}
\end{minipage}\hfill
\begin{minipage}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{shap_plots/mesh_torchlstm_val_tokyo_beeswarm.png}
    \caption*{Tokyo Mesh LSTM}
\end{minipage}\hfill
\caption{City-split SHAP beeswarms for ward LightGBM, mesh Random Forest, and LSTM variants. Momentum dominates; Sendai shows wider spreads and mild negative skew where data are sparse. Mesh LSTM SHAP uses Tokyo validation and overall test (no Sendai-specific mesh LSTM SHAP available).}
\label{fig:shap-beeswarm}
\end{figure*}

Feature dominance is consistent across all model families (Linear Regression, Random Forest, LightGBM, and LSTM). As shown in Fig.~\ref{fig:shap-beeswarm}, momentum features—specifically the four-quarter moving average and lags—dominate with the widest SHAP spreads. In contrast, structural signals (age, size, counts) and spatial identifiers cluster near zero, implying they offer limited marginal value once price history is known. Effectively, these models behave as compact autoregressors, where predictive power stems from recent trajectories rather than static location labels.

Second-order effects reveal how models handle sparsity. While structural features generally add little (e.g., Ward LightGBM ignores average age), the Mesh Random Forest uses the \texttt{WardHedonicIndex} and missingness flags to stabilize predictions when local mesh history is thin. Geographically, this uncertainty is visible in Sendai, where wider SHAP spreads and negative skew in the \texttt{QoQ} feature reflect the higher volatility of regional markets compared to the tighter clusters of Tokyo. Finally, the LSTM SHAP values mirror the tree-based rankings, confirming that the deep learning model is largely relearning the same momentum signals as the simpler architectures.

\section{Conclusions and Discussion}
Hedonic Forecast shows that a clean hedonic pipeline combined with lightweight tree-based models can effectively close the three-month publication gap in Japan's official indices. By achieving Ward baselines of 18.6k JPY/m² and mesh ensembles of 22.6k JPY/m², we demonstrate that expensive, complex datasets are not strictly necessary for high-accuracy forecasting.

Our SHAP analysis provides the strongest argument for this "lean" approach. The consistent dominance of momentum features (moving averages and lags) across all model architectures implies that price history itself contains the majority of the signal required for near-term prediction. Complex deep learning approaches like LSTMs, while powerful in other domains, largely effectively "re-discover" these momentum terms without adding sufficient marginal value to justify their opacity and computational cost in this specific domain.

Furthermore, the interpretable nature of our pipeline revealed critical spatial heterogeneities. The discovery of the "Sendai Penalty"—where regional markets punish declining momentum far more severely than Tokyo, offers actionable risk intelligence to developers and investors.

Limitations remain. The MLIT data omits micro-amenities, forcing models to lean heavily on lags; richer location signals from REINS or SUUMO would help but introduce privacy and cost barriers. Additionally, the instability of LightGBM on Sendai's smaller sample highlights the need for adaptive model selection in regional deployment. Future work will focus on integrating spatial autoregressive features to better capture neighborhood spillovers and expanding the pipeline to other regions to test the generalizability of these findings.

\noindent All team members contributed equally to all aspects of the project and report writing.
\bibliographystyle{apalike}
\begin{thebibliography}{99}

\bibitem{mlit_data}
Ministry of Land, Infrastructure, Transport and Tourism (MLIT). (2005--present). \textit{Real Estate Information Library}. Retrieved from \url{https://www.reinfolib.mlit.go.jp/}

\bibitem{jrppi2020}
Ministry of Land, Infrastructure, Transport and Tourism, Real Estate and Construction Economy Bureau. (2020). \textit{Methodology of JRPPI: Japan Residential Property Price Index}. Retrieved from \url{https://www.mlit.go.jp/common/001360414.pdf}

\bibitem{jrppi_timelag}
Ministry of Land, Infrastructure, Transport and Tourism. (2015). \textit{Japan Residential Property Price Index and Residential Transaction Volume (August 2015)}. See p.~6: ``Time lag: About 3 months'' and coverage by geography. Retrieved from \url{https://www.mlit.go.jp/common/001110934.pdf}

\bibitem{yoshida2024}
Yoshida, T., \& Seya, H. (2024). Spatial prediction of apartment rent using regression-based and machine learning-based approaches with a large dataset. \textit{The Journal of Real Estate Finance and Economics}, \textit{69}(1), 1--28. \url{https://doi.org/10.1007/s11146-022-09929-6}

\bibitem{lstm2017}
Chen, X., Wei, L., \& Xu, J. (2017). House price prediction using LSTM. \textit{arXiv preprint arXiv:1709.08432}. \url{https://arxiv.org/abs/1709.08432}

\bibitem{hyndman2002ets}
Hyndman, R. J., Koehler, A. B., Snyder, R. D., \& Grose, S. (2002). A state space framework for automatic forecasting using exponential smoothing methods. \textit{International Journal of Forecasting}, \textit{18}(3), 439--454. \url{https://www.sciencedirect.com/science/article/pii/S0169207001001108}

\bibitem{c}
Haque, D. (2024). Transforming Japan real estate. \textit{arXiv preprint arXiv:2405.20715}. \url{https://arxiv.org/abs/2405.20715}

\bibitem{transformers_survey}
Wen, Q., Zhou, T., Zhang, C., Chen, W., Ma, Z., Yan, J., \& Sun, L. (2023). Transformers in time series: A survey. In \textit{Proceedings of IJCAI 2023} (Survey Track). \url{https://www.ijcai.org/proceedings/2023/0759.pdf}

\bibitem{spatiotemporal2023}
Muto, S., Sugasawa, S., \& Suzuki, M. (2023). Hedonic real estate price estimation with the spatiotemporal geostatistical model. \textit{Journal of Spatial Econometrics}. \url{https://link.springer.com/article/10.1007/s43071-023-00039-w}

\bibitem{jsai2022}
Mizuho Research \& Technologies, Ltd. (2022). \textit{Prediction of land prices using machine learning with mesh-based neighbor features}. JSAI Special Interest Group on Financial Informatics (SIG-FIN-029-61). Retrieved from \url{https://www.jstage.jst.go.jp/article/jsaisigtwo/2022/FIN-029/2022_61/_pdf}

\bibitem{stats_mesh}
Statistics Bureau of Japan. (n.d.). \textit{The Standard Grid Square and the Grid Square Code used for the Statistics}. Retrieved from \url{https://www.stat.go.jp/english/data/mesh/02.html}

\bibitem{jshis_250m}
National Research Institute for Earth Science and Disaster Resilience (NIED). (n.d.). \textit{What is the 250m-mesh code?} J-SHIS FAQ. Retrieved from \url{https://www.j-shis.bosai.go.jp/en/faq-250mmesh}

\bibitem{hitotsubashi2023}
Otsuki, K. (2023). \textit{A Study on Data-Cleansing Methods and Model-Update Algorithms for Real Estate Price Forecasting Models} [Doctoral dissertation, Hitotsubashi University]. Hitotsubashi University Repository. \url{https://hit-u.repo.nii.ac.jp/records/2048234}

\bibitem{shap2017}
Lundberg, S. M., \& Lee, S.-I. (2017). A unified approach to interpreting model predictions. In \textit{Advances in Neural Information Processing Systems 30} (pp. 4765--4774). \url{https://arxiv.org/abs/1705.07874}

\bibitem{lundberg2020nmi}
Lundberg, S. M., Erion, G., Chen, H., DeGrave, A., Prutkin, J. M., Nair, B., Katz, R., Himmelfarb, J., Bansal, N., \& Lee, S.-I. (2020). From local explanations to global understanding with explainable AI for trees. \textit{Nature Machine Intelligence}, \textit{2}(1), 56--67. \url{https://doi.org/10.1038/s42256-019-0138-9}

\bibitem{shap_lstm2025}
Sen, D., Deora, B. S., \& Vaishnav, A. (2025). Explainable deep learning for time series analysis: Integrating SHAP and LIME in LSTM-based models. \textit{Journal of Information Systems Engineering and Management}, \textit{10}(16s). \url{https://jisem-journal.com/index.php/journal/article/view/2627}

\bibitem{multivariate_lstm2022}
Kuber, V., Yadav, D., \& Yadav, A. K. (2022). Univariate and multivariate LSTM model for short-term stock market prediction. \textit{arXiv preprint arXiv:2205.06673}. \url{https://arxiv.org/abs/2205.06673}

\bibitem{hyndman2006}
Hyndman, R. J., \& Koehler, A. B. (2006). Another look at measures of forecast accuracy. \textit{International Journal of Forecasting}, \textit{22}(4), 679--688. \url{https://robjhyndman.com/papers/mase.pdf}

\bibitem{tashman2000}
Tashman, L. J. (2000). Out-of-sample tests of forecasting accuracy: An analysis and review. \textit{International Journal of Forecasting}, \textit{16}(4), 437--450. \url{https://www.sciencedirect.com/science/article/pii/S0169207000000650}

\bibitem{peng2022}
Peng, Z., \& Inoue, R. (2022). Identifying multiple scales of spatial heterogeneity in housing prices based on eigenvector spatial filtering approaches. \textit{ISPRS International Journal of Geo-Information}, \textit{11}(5), 283. \url{https://doi.org/10.3390/ijgi11050283}

\end{thebibliography}vi

\appendix
\section{Appendix}
\setcounter{table}{0}
\setcounter{figure}{0}
\renewcommand{\thetable}{A\arabic{table}}
\renewcommand{\thefigure}{A\arabic{figure}}
\subsection{Efficiency Tables}

Tables~\ref{tab:ward-efficiency} and \ref{tab:mesh-efficiency} report inference-time scaling for the classical models (Linear Regression, Random Forest, LightGBM) at 25\%, 50\%, 75\%, and 100\% of training data. LSTM timing is excluded because sequence training and SHAP export are orders slower and hardware-dependent.

\newpage{}
\begin{table*}[t]
    \centering
    \scriptsize
    \caption{Inference-time efficiency scaling for ward-level models across training fractions.}
    \label{tab:ward-efficiency}
    \begin{tabular}{lcccccccc}
    \toprule
    Model & Fraction & Train Rows & Val Rows & Test Rows & Infer Time (s) & Test MAE & Test RMSE & $R^2$ \\
    \midrule
    LinearRegression & 0.25 & 377  & 224 & 364 & 0.0011 & 104{,}324 & 109{,}369 & 0.937 \\
    RandomForest     & 0.25 & 377  & 224 & 364 & 0.0450 & 88{,}365  & 170{,}694 & 0.846 \\
    LightGBM         & 0.25 & 377  & 224 & 364 & 0.0045 & 106{,}527 & 192{,}861 & 0.804 \\
    \midrule
    LinearRegression & 0.50 & 754  & 224 & 364 & 0.0015 & 24{,}371  & 36{,}728  & 0.993 \\
    RandomForest     & 0.50 & 754  & 224 & 364 & 0.0436 & 87{,}944  & 171{,}270 & 0.845 \\
    LightGBM         & 0.50 & 754  & 224 & 364 & 0.0049 & 111{,}882 & 206{,}757 & 0.774 \\
    \midrule
    LinearRegression & 0.75 & 1{,}131 & 224 & 364 & 0.0016 & 19{,}298  & 30{,}074  & 0.995 \\
    RandomForest     & 0.75 & 1{,}131 & 224 & 364 & 0.0490 & 77{,}972  & 167{,}894 & 0.851 \\
    LightGBM         & 0.75 & 1{,}131 & 224 & 364 & 0.0047 & 76{,}600  & 169{,}087 & 0.849 \\
    \midrule
    LinearRegression & 1.00 & 1{,}507 & 224 & 364 & 0.0015 & 18{,}595  & 29{,}651  & 0.995 \\
    RandomForest     & 1.00 & 1{,}507 & 224 & 364 & 0.0808 & 44{,}763  & 113{,}298 & 0.932 \\
    LightGBM         & 1.00 & 1{,}507 & 224 & 364 & 0.0045 & 48{,}039  & 116{,}653 & 0.928 \\
    \bottomrule
    \end{tabular}
\end{table*}

\begin{table*}[t]
    \centering
    \scriptsize
    \caption{Inference-time efficiency scaling for mesh-level models across training fractions.}
    \label{tab:mesh-efficiency}
    \begin{tabular}{lcccccccc}
    \hline
    Model & Fraction & Train Rows & Val Rows & Test Rows & Infer Time (s) & Test MAE & Test RMSE & $R^2$ \\
    \hline
    LinearRegression & 0.25 & 6{,}705 & 4{,}188 & 6{,}770 & 0.0020 & 97{,}903 & 210{,}560 & 0.858 \\
    RandomForest     & 0.25 & 6{,}705 & 4{,}188 & 6{,}770 & 0.099 & 45{,}518 & 168{,}424 & 0.909 \\
    LightGBM         & 0.25 & 6{,}705 & 4{,}188 & 6{,}770 & 0.058 & 45{,}942 & 167{,}110 & 0.911 \\
    \hline
    LinearRegression & 0.50 & 13{,}409 & 4{,}188 & 6{,}770 & 0.0022 & 85{,}203 & 203{,}366 & 0.868 \\
    RandomForest     & 0.50 & 13{,}409 & 4{,}188 & 6{,}770 & 0.082 & 33{,}971 & 152{,}441 & 0.926 \\
    LightGBM         & 0.50 & 13{,}409 & 4{,}188 & 6{,}770 & 0.049 & 36{,}607 & 134{,}185 & 0.942 \\
    \hline
    LinearRegression & 0.75 & 20{,}114 & 4{,}188 & 6{,}770 & 0.0017 & 87{,}921 & 199{,}769 & 0.872 \\
    RandomForest     & 0.75 & 20{,}114 & 4{,}188 & 6{,}770 & 0.176 & 25{,}335 & 102{,}214 & 0.967 \\
    LightGBM         & 0.75 & 20{,}114 & 4{,}188 & 6{,}770 & 0.049 & 27{,}902 & 124{,}372 & 0.951 \\
    \hline
    LinearRegression & 1.00 & 26{,}818 & 4{,}188 & 6{,}770 & 0.0028 & 83{,}896 & 197{,}137 & 0.876 \\
    RandomForest     & 1.00 & 26{,}818 & 4{,}188 & 6{,}770 & 0.142 & 24{,}303 & 126{,}879 & 0.949 \\
    LightGBM         & 1.00 & 26{,}818 & 4{,}188 & 6{,}770 & 0.047 & 27{,}702 & 126{,}969 & 0.948 \\
    \hline
    \end{tabular}
\end{table*}

\end{document}

