{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a970c436",
      "metadata": {},
      "source": [
        "# 03 Models v2 - Forecasting + Interpretability"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e588aa82",
      "metadata": {},
      "source": [
        "This notebook rebuilds the modeling workflow with three goals:\n",
        "- reproducible ward/mesh baselines (Linear Regression, Random Forest, LightGBM)\n",
        "- SHAP exports that Streamlit can read for global + local explainability\n",
        "- a lightweight PyTorch LSTM baseline for wards, giving us a neural alternative without bloating runtime\n",
        "\n",
        "Running everything end-to-end produces the CSV/pickle artifacts consumed by the dashboard."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "b50818eb",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# sets up imports, configuration, and helper utilities.\n",
        "from __future__ import annotations\n",
        "\n",
        "import json\n",
        "import math\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "from typing import Dict, List\n",
        "\n",
        "import joblib\n",
        "import lightgbm as lgb\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import shap\n",
        "import torch\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "NOTEBOOK_DIR = Path.cwd()\n",
        "DATA_DIR = NOTEBOOK_DIR\n",
        "if not (DATA_DIR / \"main_features.parquet\").exists():\n",
        "    candidate = NOTEBOOK_DIR / \"test_notebooks\"\n",
        "    if (candidate / \"main_features.parquet\").exists():\n",
        "        DATA_DIR = candidate\n",
        "\n",
        "OUTPUT_DIR = DATA_DIR\n",
        "SHAP_DIR = DATA_DIR / \"shap_outputs\"\n",
        "for folder in [OUTPUT_DIR, SHAP_DIR]:\n",
        "    folder.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "TRAIN_END = \"2019-Q4\"\n",
        "VAL_END = \"2021-Q4\"\n",
        "\n",
        "SEQ_LEN = 8\n",
        "BATCH_SIZE = 128\n",
        "MAX_EPOCHS = 75\n",
        "LEARNING_RATE = 1e-3\n",
        "EARLY_STOPPING_PATIENCE = 8\n",
        "TARGET_SCALE = 1_000_000  # scale LSTM targets to keep gradients stable\n",
        "TREE_MODELS = {\"RandomForest\", \"LightGBM\"}\n",
        "\n",
        "\n",
        "def period_to_order(period_key: str) -> int:\n",
        "    year, quarter = period_key.split(\"-Q\")\n",
        "    return int(year) * 4 + (int(quarter) - 1)\n",
        "\n",
        "\n",
        "def period_to_float(period_key: str) -> float:\n",
        "    return period_to_order(period_key) / 4.0\n",
        "\n",
        "\n",
        "def evaluate_sets(y_true, y_pred) -> Dict[str, float]:\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_pred = np.asarray(y_pred)\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    return {\"mae\": mae, \"rmse\": float(np.sqrt(mse)), \"r2\": r2_score(y_true, y_pred)}\n",
        "\n",
        "\n",
        "def temporal_split(df: pd.DataFrame, train_end: str, val_end: str):\n",
        "    order_series = df[\"PeriodKey\"].apply(period_to_order)\n",
        "    train_mask = order_series <= period_to_order(train_end)\n",
        "    val_mask = (order_series > period_to_order(train_end)) & (order_series <= period_to_order(val_end))\n",
        "    test_mask = order_series > period_to_order(val_end)\n",
        "    return df[train_mask], df[val_mask], df[test_mask]\n",
        "\n",
        "\n",
        "def add_temporal_features(df: pd.DataFrame, group_col: str, target_col: str) -> pd.DataFrame:\n",
        "    df = df.sort_values([group_col, \"PeriodKey\"]).copy()\n",
        "    df[\"PeriodNum\"] = df[\"PeriodKey\"].astype(str).apply(period_to_float)\n",
        "    for lag in [1, 4]:\n",
        "        df[f\"{target_col}_lag{lag}\"] = df.groupby(group_col)[target_col].shift(lag)\n",
        "    df[f\"{target_col}_growth_qoq\"] = (df[target_col] / df[f\"{target_col}_lag1\"] - 1) * 100\n",
        "    df[f\"{target_col}_growth_yoy\"] = (df[target_col] / df[f\"{target_col}_lag4\"] - 1) * 100\n",
        "    df[f\"{target_col}_ma4q\"] = (\n",
        "        df.groupby(group_col)[target_col]\n",
        "        .rolling(window=4, min_periods=1)\n",
        "        .mean()\n",
        "        .reset_index(level=0, drop=True)\n",
        "    )\n",
        "    df[f\"{target_col}_std4q\"] = (\n",
        "        df.groupby(group_col)[target_col]\n",
        "        .rolling(window=4, min_periods=1)\n",
        "        .std()\n",
        "        .reset_index(level=0, drop=True)\n",
        "    )\n",
        "    return df\n",
        "\n",
        "\n",
        "MODEL_FACTORIES = {\n",
        "    \"LinearRegression\": lambda: LinearRegression(),\n",
        "    \"RandomForest\": lambda: RandomForestRegressor(n_estimators=300, random_state=42, n_jobs=-1),\n",
        "    \"LightGBM\": lambda: lgb.LGBMRegressor(\n",
        "        n_estimators=600,\n",
        "        learning_rate=0.05,\n",
        "        num_leaves=31,\n",
        "        subsample=0.9,\n",
        "        colsample_bytree=0.9,\n",
        "        random_state=42,\n",
        "    ),\n",
        "}\n",
        "\n",
        "TRAIN_END_ORDER = period_to_order(TRAIN_END)\n",
        "VAL_END_ORDER = period_to_order(VAL_END)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "d6d7ef7c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded main_features.csv for sharing; modeling uses the Parquet table.\n",
            "Transactions loaded: 485,093\n",
            "Mesh quarters loaded: 26,842\n",
            "Time span: 2005-Q3 -> 2025-Q1\n",
            "Hedonic index CSVs not found; skipping hedonic comparison plots.\n"
          ]
        }
      ],
      "source": [
        "# loads cleaned feature tables and hedonic references used downstream.\n",
        "MAIN_FEATURES_PATH = DATA_DIR / \"main_features.parquet\"\n",
        "MAIN_FEATURES_CSV = DATA_DIR / \"main_features.csv\"\n",
        "MESH_FEATURES_PATH = DATA_DIR / \"mesh_quarter_features.csv\"\n",
        "\n",
        "if not MAIN_FEATURES_PATH.exists():\n",
        "    raise FileNotFoundError(MAIN_FEATURES_PATH)\n",
        "if not MESH_FEATURES_PATH.exists():\n",
        "    raise FileNotFoundError(MESH_FEATURES_PATH)\n",
        "\n",
        "main_df = pd.read_parquet(MAIN_FEATURES_PATH)\n",
        "if MAIN_FEATURES_CSV.exists():\n",
        "    print(\"Loaded main_features.csv for sharing; modeling uses the Parquet table.\")\n",
        "mesh_panel_raw = pd.read_csv(MESH_FEATURES_PATH)\n",
        "\n",
        "main_df[\"Mesh250m\"] = main_df[\"Mesh250m\"].astype(str)\n",
        "main_df.loc[main_df[\"Mesh250m\"].str.lower() == \"nan\", \"Mesh250m\"] = np.nan\n",
        "main_df[\"WardName\"] = main_df[\"Municipality_en\"].fillna(main_df[\"Municipality\"]).fillna(\"Unknown\")\n",
        "\n",
        "mesh_panel_raw[\"Mesh250m\"] = mesh_panel_raw[\"Mesh250m\"].astype(str)\n",
        "mesh_panel_raw.loc[mesh_panel_raw[\"Mesh250m\"].str.lower() == \"nan\", \"Mesh250m\"] = np.nan\n",
        "\n",
        "hedonic_paths = {\n",
        "    \"overall\": DATA_DIR / \"hedonic_index_overall.csv\",\n",
        "    \"ward_full\": DATA_DIR / \"hedonic_index_by_ward.csv\",\n",
        "    \"mesh_full\": DATA_DIR / \"hedonic_index_by_mesh.csv\",\n",
        "    \"ward_train\": DATA_DIR / \"hedonic_index_by_ward_trainmodel.csv\",\n",
        "    \"mesh_train\": DATA_DIR / \"hedonic_index_by_mesh_trainmodel.csv\",\n",
        "}\n",
        "\n",
        "\n",
        "def _load_optional_csv(path: Path) -> pd.DataFrame:\n",
        "    if path.exists():\n",
        "        df = pd.read_csv(path)\n",
        "        if \"PeriodKey\" in df.columns:\n",
        "            df[\"PeriodKey\"] = df[\"PeriodKey\"].astype(str)\n",
        "        return df\n",
        "    return pd.DataFrame()\n",
        "\n",
        "\n",
        "hedonic_index_overall = _load_optional_csv(hedonic_paths[\"overall\"])\n",
        "ward_hedonic_index_full = _load_optional_csv(hedonic_paths[\"ward_full\"])\n",
        "mesh_hedonic_index_full = _load_optional_csv(hedonic_paths[\"mesh_full\"])\n",
        "ward_hedonic_index = _load_optional_csv(hedonic_paths[\"ward_train\"])\n",
        "mesh_hedonic_index = _load_optional_csv(hedonic_paths[\"mesh_train\"])\n",
        "\n",
        "HEDONIC_AVAILABLE = not hedonic_index_overall.empty\n",
        "\n",
        "print(f\"Transactions loaded: {len(main_df):,}\")\n",
        "print(f\"Mesh quarters loaded: {len(mesh_panel_raw):,}\")\n",
        "print(f\"Time span: {main_df['PeriodKey'].min()} -> {main_df['PeriodKey'].max()}\")\n",
        "if not HEDONIC_AVAILABLE:\n",
        "    print(\"Hedonic index CSVs not found; skipping hedonic comparison plots.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "201f6711",
      "metadata": {},
      "source": [
        "## Ward-level aggregation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "6674532c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ward panel shape: (2207, 27)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Ward</th>\n",
              "      <th>PeriodKey</th>\n",
              "      <th>MedianPriceSqM</th>\n",
              "      <th>MeanPriceSqM</th>\n",
              "      <th>StdPriceSqM</th>\n",
              "      <th>MedianTotalPrice</th>\n",
              "      <th>AvgBuildingAge</th>\n",
              "      <th>AvgArea</th>\n",
              "      <th>ActiveMeshes</th>\n",
              "      <th>TransactionCount</th>\n",
              "      <th>...</th>\n",
              "      <th>MedianPriceSqM_growth_qoq</th>\n",
              "      <th>MedianPriceSqM_growth_yoy</th>\n",
              "      <th>MedianPriceSqM_ma4q</th>\n",
              "      <th>MedianPriceSqM_std4q</th>\n",
              "      <th>Q_2</th>\n",
              "      <th>Q_3</th>\n",
              "      <th>Q_4</th>\n",
              "      <th>TimeTrend</th>\n",
              "      <th>Ward_encoded</th>\n",
              "      <th>Order</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Adachi Ward</td>\n",
              "      <td>2005-Q3</td>\n",
              "      <td>272077.922078</td>\n",
              "      <td>317513.105806</td>\n",
              "      <td>211710.128325</td>\n",
              "      <td>28500000.0</td>\n",
              "      <td>12.370166</td>\n",
              "      <td>172.445255</td>\n",
              "      <td>20</td>\n",
              "      <td>274</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>272077.922078</td>\n",
              "      <td>NaN</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Adachi Ward</td>\n",
              "      <td>2005-Q4</td>\n",
              "      <td>302000.000000</td>\n",
              "      <td>331062.433878</td>\n",
              "      <td>148225.544631</td>\n",
              "      <td>32000000.0</td>\n",
              "      <td>10.218750</td>\n",
              "      <td>152.871257</td>\n",
              "      <td>18</td>\n",
              "      <td>334</td>\n",
              "      <td>...</td>\n",
              "      <td>10.997613</td>\n",
              "      <td>NaN</td>\n",
              "      <td>287038.961039</td>\n",
              "      <td>21158.104206</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>8023</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Adachi Ward</td>\n",
              "      <td>2006-Q1</td>\n",
              "      <td>296666.666667</td>\n",
              "      <td>330761.524037</td>\n",
              "      <td>179748.518250</td>\n",
              "      <td>28000000.0</td>\n",
              "      <td>12.205000</td>\n",
              "      <td>211.641304</td>\n",
              "      <td>18</td>\n",
              "      <td>276</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.766004</td>\n",
              "      <td>NaN</td>\n",
              "      <td>290248.196248</td>\n",
              "      <td>15960.271260</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>8024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Adachi Ward</td>\n",
              "      <td>2006-Q2</td>\n",
              "      <td>316025.641026</td>\n",
              "      <td>338663.621605</td>\n",
              "      <td>134662.595791</td>\n",
              "      <td>30000000.0</td>\n",
              "      <td>10.297561</td>\n",
              "      <td>195.388489</td>\n",
              "      <td>17</td>\n",
              "      <td>278</td>\n",
              "      <td>...</td>\n",
              "      <td>6.525497</td>\n",
              "      <td>NaN</td>\n",
              "      <td>296692.557443</td>\n",
              "      <td>18328.647993</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>8025</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Adachi Ward</td>\n",
              "      <td>2006-Q3</td>\n",
              "      <td>343167.701863</td>\n",
              "      <td>381259.114252</td>\n",
              "      <td>369920.460052</td>\n",
              "      <td>31000000.0</td>\n",
              "      <td>10.588745</td>\n",
              "      <td>152.731544</td>\n",
              "      <td>17</td>\n",
              "      <td>298</td>\n",
              "      <td>...</td>\n",
              "      <td>8.588563</td>\n",
              "      <td>26.128463</td>\n",
              "      <td>314465.002389</td>\n",
              "      <td>20804.146935</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>8026</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 27 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          Ward PeriodKey  MedianPriceSqM   MeanPriceSqM    StdPriceSqM  \\\n",
              "0  Adachi Ward   2005-Q3   272077.922078  317513.105806  211710.128325   \n",
              "1  Adachi Ward   2005-Q4   302000.000000  331062.433878  148225.544631   \n",
              "2  Adachi Ward   2006-Q1   296666.666667  330761.524037  179748.518250   \n",
              "3  Adachi Ward   2006-Q2   316025.641026  338663.621605  134662.595791   \n",
              "4  Adachi Ward   2006-Q3   343167.701863  381259.114252  369920.460052   \n",
              "\n",
              "   MedianTotalPrice  AvgBuildingAge     AvgArea  ActiveMeshes  \\\n",
              "0        28500000.0       12.370166  172.445255            20   \n",
              "1        32000000.0       10.218750  152.871257            18   \n",
              "2        28000000.0       12.205000  211.641304            18   \n",
              "3        30000000.0       10.297561  195.388489            17   \n",
              "4        31000000.0       10.588745  152.731544            17   \n",
              "\n",
              "   TransactionCount  ...  MedianPriceSqM_growth_qoq  \\\n",
              "0               274  ...                        NaN   \n",
              "1               334  ...                  10.997613   \n",
              "2               276  ...                  -1.766004   \n",
              "3               278  ...                   6.525497   \n",
              "4               298  ...                   8.588563   \n",
              "\n",
              "   MedianPriceSqM_growth_yoy  MedianPriceSqM_ma4q  MedianPriceSqM_std4q  \\\n",
              "0                        NaN        272077.922078                   NaN   \n",
              "1                        NaN        287038.961039          21158.104206   \n",
              "2                        NaN        290248.196248          15960.271260   \n",
              "3                        NaN        296692.557443          18328.647993   \n",
              "4                  26.128463        314465.002389          20804.146935   \n",
              "\n",
              "     Q_2    Q_3    Q_4  TimeTrend  Ward_encoded  Order  \n",
              "0  False   True  False          0             0   8022  \n",
              "1  False  False   True          1             0   8023  \n",
              "2  False  False  False          2             0   8024  \n",
              "3   True  False  False          3             0   8025  \n",
              "4  False   True  False          4             0   8026  \n",
              "\n",
              "[5 rows x 27 columns]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# aggregates ward-level metrics and engineers time features.\n",
        "ward_panel = main_df.groupby([\"WardName\", \"PeriodKey\"], dropna=False).agg({\n",
        "    \"PricePerSqM\": [\"median\", \"mean\", \"std\"],\n",
        "    \"TradePriceValue\": \"median\",\n",
        "    \"BuildingAge\": \"mean\",\n",
        "    \"AreaSqM\": \"mean\",\n",
        "    \"Mesh250m\": \"nunique\",\n",
        "}).reset_index()\n",
        "ward_panel.columns = [\n",
        "    \"Ward\", \"PeriodKey\", \"MedianPriceSqM\", \"MeanPriceSqM\", \"StdPriceSqM\",\n",
        "    \"MedianTotalPrice\", \"AvgBuildingAge\", \"AvgArea\", \"ActiveMeshes\"\n",
        "]\n",
        "ward_counts = (\n",
        "    main_df.groupby([\"WardName\", \"PeriodKey\"], dropna=False)\n",
        "    .size()\n",
        "    .reset_index(name=\"TransactionCount\")\n",
        "    .rename(columns={\"WardName\": \"Ward\"})\n",
        ")\n",
        "ward_panel = ward_panel.merge(ward_counts, on=[\"Ward\", \"PeriodKey\"], how=\"left\")\n",
        "ward_coordinates = (\n",
        "    main_df.dropna(subset=[\"Latitude\", \"Longitude\"])\n",
        "    .groupby(\"WardName\")\n",
        "    .agg(WardLat=(\"Latitude\", \"median\"), WardLon=(\"Longitude\", \"median\"))\n",
        "    .reset_index()\n",
        "    .rename(columns={\"WardName\": \"Ward\"})\n",
        ")\n",
        "ward_panel = ward_panel.merge(ward_coordinates, on=\"Ward\", how=\"left\")\n",
        "\n",
        "if not ward_hedonic_index_full.empty and {\"Ward\", \"PeriodKey\", \"WardHedonicIndexFull\"}.issubset(ward_hedonic_index_full.columns):\n",
        "    ward_panel = ward_panel.merge(\n",
        "        ward_hedonic_index_full[[\"Ward\", \"PeriodKey\", \"WardHedonicIndexFull\"]],\n",
        "        on=[\"Ward\", \"PeriodKey\"],\n",
        "        how=\"left\",\n",
        "    )\n",
        "else:\n",
        "    ward_panel[\"WardHedonicIndexFull\"] = np.nan\n",
        "\n",
        "if not ward_hedonic_index.empty and {\"Ward\", \"PeriodKey\", \"WardHedonicIndex\"}.issubset(ward_hedonic_index.columns):\n",
        "    ward_panel = ward_panel.merge(\n",
        "        ward_hedonic_index[[\"Ward\", \"PeriodKey\", \"WardHedonicIndex\"]],\n",
        "        on=[\"Ward\", \"PeriodKey\"],\n",
        "        how=\"left\",\n",
        "    )\n",
        "else:\n",
        "    if \"WardHedonicIndex\" not in ward_panel.columns:\n",
        "        ward_panel[\"WardHedonicIndex\"] = np.nan\n",
        "\n",
        "ward_panel = add_temporal_features(ward_panel, \"Ward\", \"MedianPriceSqM\")\n",
        "ward_panel[\"Quarter\"] = ward_panel[\"PeriodKey\"].str.extract(r\"Q(\\d)\")[0].astype(int)\n",
        "ward_panel = pd.get_dummies(ward_panel, columns=[\"Quarter\"], prefix=\"Q\", drop_first=True)\n",
        "ward_panel[\"TimeTrend\"] = ward_panel.groupby(\"Ward\").cumcount()\n",
        "ward_encoder = LabelEncoder()\n",
        "ward_panel[\"Ward_encoded\"] = ward_encoder.fit_transform(ward_panel[\"Ward\"].fillna(\"Unknown\"))\n",
        "ward_panel[\"Order\"] = ward_panel[\"PeriodKey\"].apply(period_to_order)\n",
        "\n",
        "print(f\"Ward panel shape: {ward_panel.shape}\")\n",
        "ward_panel.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e9c1683",
      "metadata": {},
      "source": [
        "## Mesh-level aggregation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "26a08714",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mesh panel shape: (41072, 31)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Mesh250m</th>\n",
              "      <th>PeriodKey</th>\n",
              "      <th>mesh_transaction_count</th>\n",
              "      <th>mesh_median_ppsqm</th>\n",
              "      <th>mesh_mean_ppsqm</th>\n",
              "      <th>mesh_price_std</th>\n",
              "      <th>mesh_price_iqr</th>\n",
              "      <th>mesh_avg_age</th>\n",
              "      <th>mesh_avg_area</th>\n",
              "      <th>PeriodNum</th>\n",
              "      <th>...</th>\n",
              "      <th>WardHedonicIndex</th>\n",
              "      <th>WardHedonicIndexFull</th>\n",
              "      <th>WardLat</th>\n",
              "      <th>WardLon</th>\n",
              "      <th>MeshLat</th>\n",
              "      <th>MeshLon</th>\n",
              "      <th>MeshHedonicIndexFull</th>\n",
              "      <th>MeshHedonicIndex</th>\n",
              "      <th>Ward_encoded</th>\n",
              "      <th>Order</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>53392546</td>\n",
              "      <td>2005-Q3</td>\n",
              "      <td>3</td>\n",
              "      <td>446153.846154</td>\n",
              "      <td>543728.283085</td>\n",
              "      <td>294852.624070</td>\n",
              "      <td>282484.498450</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>3368.000000</td>\n",
              "      <td>2005.50</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>35.572571</td>\n",
              "      <td>139.708489</td>\n",
              "      <td>35.540512</td>\n",
              "      <td>139.707448</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>17</td>\n",
              "      <td>8022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>53392546</td>\n",
              "      <td>2005-Q4</td>\n",
              "      <td>1</td>\n",
              "      <td>550000.000000</td>\n",
              "      <td>550000.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>60.000000</td>\n",
              "      <td>2005.75</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>35.572571</td>\n",
              "      <td>139.708489</td>\n",
              "      <td>35.540512</td>\n",
              "      <td>139.707448</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>17</td>\n",
              "      <td>8023</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>53392546</td>\n",
              "      <td>2006-Q3</td>\n",
              "      <td>11</td>\n",
              "      <td>950000.000000</td>\n",
              "      <td>815609.239246</td>\n",
              "      <td>302013.869517</td>\n",
              "      <td>480000.000000</td>\n",
              "      <td>3.800000</td>\n",
              "      <td>39.545455</td>\n",
              "      <td>2006.50</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>35.572571</td>\n",
              "      <td>139.708489</td>\n",
              "      <td>35.540512</td>\n",
              "      <td>139.707448</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>17</td>\n",
              "      <td>8026</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>53392546</td>\n",
              "      <td>2007-Q1</td>\n",
              "      <td>6</td>\n",
              "      <td>557017.543860</td>\n",
              "      <td>576573.280521</td>\n",
              "      <td>243349.016575</td>\n",
              "      <td>365744.891074</td>\n",
              "      <td>10.500000</td>\n",
              "      <td>190.833333</td>\n",
              "      <td>2007.00</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>35.572571</td>\n",
              "      <td>139.708489</td>\n",
              "      <td>35.540512</td>\n",
              "      <td>139.707448</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>17</td>\n",
              "      <td>8028</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>53392546</td>\n",
              "      <td>2007-Q3</td>\n",
              "      <td>3</td>\n",
              "      <td>600000.000000</td>\n",
              "      <td>627777.777778</td>\n",
              "      <td>259450.987307</td>\n",
              "      <td>258333.333333</td>\n",
              "      <td>8.666667</td>\n",
              "      <td>48.333333</td>\n",
              "      <td>2007.50</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>35.572571</td>\n",
              "      <td>139.708489</td>\n",
              "      <td>35.540512</td>\n",
              "      <td>139.707448</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>17</td>\n",
              "      <td>8030</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 31 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Mesh250m PeriodKey  mesh_transaction_count  mesh_median_ppsqm  \\\n",
              "0  53392546   2005-Q3                       3      446153.846154   \n",
              "1  53392546   2005-Q4                       1      550000.000000   \n",
              "2  53392546   2006-Q3                      11      950000.000000   \n",
              "3  53392546   2007-Q1                       6      557017.543860   \n",
              "4  53392546   2007-Q3                       3      600000.000000   \n",
              "\n",
              "   mesh_mean_ppsqm  mesh_price_std  mesh_price_iqr  mesh_avg_age  \\\n",
              "0    543728.283085   294852.624070   282484.498450      3.000000   \n",
              "1    550000.000000             NaN        0.000000      2.000000   \n",
              "2    815609.239246   302013.869517   480000.000000      3.800000   \n",
              "3    576573.280521   243349.016575   365744.891074     10.500000   \n",
              "4    627777.777778   259450.987307   258333.333333      8.666667   \n",
              "\n",
              "   mesh_avg_area  PeriodNum  ...  WardHedonicIndex  WardHedonicIndexFull  \\\n",
              "0    3368.000000    2005.50  ...               NaN                   NaN   \n",
              "1      60.000000    2005.75  ...               NaN                   NaN   \n",
              "2      39.545455    2006.50  ...               NaN                   NaN   \n",
              "3     190.833333    2007.00  ...               NaN                   NaN   \n",
              "4      48.333333    2007.50  ...               NaN                   NaN   \n",
              "\n",
              "     WardLat     WardLon    MeshLat     MeshLon  MeshHedonicIndexFull  \\\n",
              "0  35.572571  139.708489  35.540512  139.707448                   NaN   \n",
              "1  35.572571  139.708489  35.540512  139.707448                   NaN   \n",
              "2  35.572571  139.708489  35.540512  139.707448                   NaN   \n",
              "3  35.572571  139.708489  35.540512  139.707448                   NaN   \n",
              "4  35.572571  139.708489  35.540512  139.707448                   NaN   \n",
              "\n",
              "   MeshHedonicIndex  Ward_encoded  Order  \n",
              "0               NaN            17   8022  \n",
              "1               NaN            17   8023  \n",
              "2               NaN            17   8026  \n",
              "3               NaN            17   8028  \n",
              "4               NaN            17   8030  \n",
              "\n",
              "[5 rows x 31 columns]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# builds mesh-level panel with ward context features.\n",
        "mesh_panel = mesh_panel_raw.copy()\n",
        "mesh_panel[\"PeriodKey\"] = mesh_panel[\"PeriodKey\"].astype(str)\n",
        "mesh_panel = add_temporal_features(mesh_panel, \"Mesh250m\", \"mesh_median_ppsqm\")\n",
        "mesh_panel[\"Quarter\"] = mesh_panel[\"PeriodKey\"].str.extract(r\"Q(\\d)\")[0].astype(int)\n",
        "mesh_panel = pd.get_dummies(mesh_panel, columns=[\"Quarter\"], prefix=\"Q\", drop_first=True)\n",
        "mesh_panel[\"TimeTrend\"] = mesh_panel.groupby(\"Mesh250m\").cumcount()\n",
        "\n",
        "mesh_to_ward = (\n",
        "    main_df[[\"Mesh250m\", \"WardName\"]]\n",
        "    .dropna(subset=[\"Mesh250m\"])\n",
        "    .drop_duplicates()\n",
        "    .rename(columns={\"WardName\": \"Ward\"})\n",
        ")\n",
        "mesh_panel = mesh_panel.merge(mesh_to_ward, on=\"Mesh250m\", how=\"left\")\n",
        "mesh_panel = mesh_panel.merge(\n",
        "    ward_panel[[\"Ward\", \"PeriodKey\", \"WardHedonicIndex\", \"WardHedonicIndexFull\", \"WardLat\", \"WardLon\"]],\n",
        "    on=[\"Ward\", \"PeriodKey\"],\n",
        "    how=\"left\",\n",
        ")\n",
        "mesh_coordinates = (\n",
        "    main_df.dropna(subset=[\"Mesh250m\", \"Latitude\", \"Longitude\"])\n",
        "    .groupby(\"Mesh250m\")\n",
        "    .agg(MeshLat=(\"Latitude\", \"median\"), MeshLon=(\"Longitude\", \"median\"))\n",
        "    .reset_index()\n",
        ")\n",
        "mesh_panel = mesh_panel.merge(mesh_coordinates, on=\"Mesh250m\", how=\"left\")\n",
        "\n",
        "if not mesh_hedonic_index_full.empty and {\"Mesh250m\", \"PeriodKey\", \"MeshHedonicIndexFull\"}.issubset(mesh_hedonic_index_full.columns):\n",
        "    mesh_panel = mesh_panel.merge(\n",
        "        mesh_hedonic_index_full[[\"Mesh250m\", \"PeriodKey\", \"MeshHedonicIndexFull\"]],\n",
        "        on=[\"Mesh250m\", \"PeriodKey\"],\n",
        "        how=\"left\",\n",
        "    )\n",
        "else:\n",
        "    mesh_panel[\"MeshHedonicIndexFull\"] = mesh_panel.get(\"MeshHedonicIndexFull\", np.nan)\n",
        "\n",
        "if not mesh_hedonic_index.empty and {\"Mesh250m\", \"PeriodKey\", \"MeshHedonicIndex\"}.issubset(mesh_hedonic_index.columns):\n",
        "    mesh_panel = mesh_panel.merge(\n",
        "        mesh_hedonic_index[[\"Mesh250m\", \"PeriodKey\", \"MeshHedonicIndex\"]],\n",
        "        on=[\"Mesh250m\", \"PeriodKey\"],\n",
        "        how=\"left\",\n",
        "    )\n",
        "else:\n",
        "    if \"MeshHedonicIndex\" not in mesh_panel.columns:\n",
        "        mesh_panel[\"MeshHedonicIndex\"] = np.nan\n",
        "\n",
        "mesh_panel[\"Ward_encoded\"] = ward_encoder.transform(mesh_panel[\"Ward\"].fillna(\"Unknown\"))\n",
        "mesh_panel[\"Order\"] = mesh_panel[\"PeriodKey\"].apply(period_to_order)\n",
        "\n",
        "print(f\"Mesh panel shape: {mesh_panel.shape}\")\n",
        "mesh_panel.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "065cf72d",
      "metadata": {},
      "source": [
        "## Train classical baselines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "8689fa0c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000113 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2413\n",
            "[LightGBM] [Info] Number of data points in the train set: 1507, number of used features: 15\n",
            "[LightGBM] [Info] Start training from score 579864.310141\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>train_mae</th>\n",
              "      <th>train_rmse</th>\n",
              "      <th>train_r2</th>\n",
              "      <th>val_mae</th>\n",
              "      <th>val_rmse</th>\n",
              "      <th>val_r2</th>\n",
              "      <th>test_mae</th>\n",
              "      <th>test_rmse</th>\n",
              "      <th>test_r2</th>\n",
              "      <th>Level</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LinearRegression</td>\n",
              "      <td>15289.386684</td>\n",
              "      <td>21997.566438</td>\n",
              "      <td>0.994241</td>\n",
              "      <td>14414.898507</td>\n",
              "      <td>19777.222355</td>\n",
              "      <td>0.997015</td>\n",
              "      <td>18594.707647</td>\n",
              "      <td>29651.119045</td>\n",
              "      <td>0.995358</td>\n",
              "      <td>Ward</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>RandomForest</td>\n",
              "      <td>3940.526974</td>\n",
              "      <td>6097.334658</td>\n",
              "      <td>0.999558</td>\n",
              "      <td>14886.734120</td>\n",
              "      <td>26010.622263</td>\n",
              "      <td>0.994837</td>\n",
              "      <td>44302.315954</td>\n",
              "      <td>112688.612153</td>\n",
              "      <td>0.932945</td>\n",
              "      <td>Ward</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>LightGBM</td>\n",
              "      <td>1117.026318</td>\n",
              "      <td>1858.730250</td>\n",
              "      <td>0.999959</td>\n",
              "      <td>15949.871880</td>\n",
              "      <td>29862.142839</td>\n",
              "      <td>0.993195</td>\n",
              "      <td>48038.703388</td>\n",
              "      <td>116652.766178</td>\n",
              "      <td>0.928145</td>\n",
              "      <td>Ward</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              Model     train_mae    train_rmse  train_r2       val_mae  \\\n",
              "0  LinearRegression  15289.386684  21997.566438  0.994241  14414.898507   \n",
              "1      RandomForest   3940.526974   6097.334658  0.999558  14886.734120   \n",
              "2          LightGBM   1117.026318   1858.730250  0.999959  15949.871880   \n",
              "\n",
              "       val_rmse    val_r2      test_mae      test_rmse   test_r2 Level  \n",
              "0  19777.222355  0.997015  18594.707647   29651.119045  0.995358  Ward  \n",
              "1  26010.622263  0.994837  44302.315954  112688.612153  0.932945  Ward  \n",
              "2  29862.142839  0.993195  48038.703388  116652.766178  0.928145  Ward  "
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# trains linear, forest, and lgbm baselines for wards and meshes.\n",
        "WARD_TARGET = \"MedianPriceSqM\"\n",
        "WARD_FEATURE_COLS = [\n",
        "    \"MedianPriceSqM_lag1\", \"MedianPriceSqM_lag4\", \"MedianPriceSqM_growth_qoq\",\n",
        "    \"MedianPriceSqM_growth_yoy\", \"MedianPriceSqM_ma4q\", \"MedianPriceSqM_std4q\",\n",
        "    \"TransactionCount\", \"AvgBuildingAge\", \"AvgArea\", \"ActiveMeshes\", \"TimeTrend\",\n",
        "    \"Ward_encoded\", \"Q_2\", \"Q_3\", \"Q_4\"\n",
        "]\n",
        "\n",
        "MESH_TARGET = \"mesh_median_ppsqm\"\n",
        "MESH_FEATURE_COLS = [\n",
        "    \"mesh_median_ppsqm_lag1\", \"mesh_median_ppsqm_lag4\", \"mesh_median_ppsqm_growth_qoq\",\n",
        "    \"mesh_median_ppsqm_growth_yoy\", \"mesh_median_ppsqm_ma4q\", \"mesh_median_ppsqm_std4q\",\n",
        "    \"mesh_transaction_count\", \"mesh_avg_age\", \"mesh_avg_area\", \"TimeTrend\",\n",
        "    \"Ward_encoded\", \"WardHedonicIndex\", \"MeshHedonicIndex\"\n",
        "]\n",
        "\n",
        "\n",
        "def prepare_model_splits(panel: pd.DataFrame, feature_cols: List[str], target_col: str):\n",
        "    feature_cols = [c for c in feature_cols if c in panel.columns]\n",
        "    train_df, val_df, test_df = temporal_split(panel.dropna(subset=[target_col]), TRAIN_END, VAL_END)\n",
        "    splits = {\n",
        "        \"train\": train_df.dropna(subset=feature_cols + [target_col]).copy(),\n",
        "        \"val\": val_df.dropna(subset=feature_cols + [target_col]).copy(),\n",
        "        \"test\": test_df.dropna(subset=feature_cols + [target_col]).copy(),\n",
        "    }\n",
        "    return splits, feature_cols\n",
        "\n",
        "\n",
        "def train_regression_models(panel: pd.DataFrame, level_name: str, target_col: str, feature_cols: List[str]):\n",
        "    splits, feature_cols = prepare_model_splits(panel, feature_cols, target_col)\n",
        "    results, prediction_frames, trained_models = [], [], {}\n",
        "\n",
        "    for name, factory in MODEL_FACTORIES.items():\n",
        "        train_df = splits[\"train\"]\n",
        "        if train_df.empty:\n",
        "            continue\n",
        "        model = factory()\n",
        "        model.fit(train_df[feature_cols], train_df[target_col])\n",
        "        preds = {split: model.predict(split_df[feature_cols]) for split, split_df in splits.items()}\n",
        "        metrics = {\n",
        "            split: evaluate_sets(split_df[target_col], preds[split])\n",
        "            for split, split_df in splits.items() if not split_df.empty\n",
        "        }\n",
        "        results.append({\n",
        "            \"Model\": name,\n",
        "            **{f\"{split}_{metric}\": values[metric] for split, values in metrics.items() for metric in [\"mae\", \"rmse\", \"r2\"]},\n",
        "        })\n",
        "        for split, split_df in splits.items():\n",
        "            if split_df.empty:\n",
        "                continue\n",
        "            payload = {\n",
        "                \"Level\": level_name,\n",
        "                \"Model\": name,\n",
        "                \"Split\": split,\n",
        "                \"PeriodKey\": split_df[\"PeriodKey\"].values,\n",
        "                \"Actual\": split_df[target_col].values,\n",
        "                \"Predicted\": preds[split],\n",
        "            }\n",
        "            for col in [\"Ward\", \"WardLat\", \"WardLon\", \"Mesh250m\", \"MeshLat\", \"MeshLon\"]:\n",
        "                payload[col] = split_df.get(col, pd.Series(np.nan, index=split_df.index)).values\n",
        "            prediction_frames.append(pd.DataFrame(payload))\n",
        "        model_path = OUTPUT_DIR / f\"{level_name.lower()}_model_{name.lower()}.pkl\"\n",
        "        joblib.dump(model, model_path)\n",
        "        trained_models[name] = model\n",
        "\n",
        "    results_df = pd.DataFrame(results)\n",
        "    results_df[\"Level\"] = level_name\n",
        "    predictions_df = pd.concat(prediction_frames, ignore_index=True) if prediction_frames else pd.DataFrame()\n",
        "    return results_df, predictions_df, trained_models, splits, feature_cols\n",
        "\n",
        "\n",
        "ward_results_df, ward_predictions_df, ward_models, ward_splits, ward_features = train_regression_models(\n",
        "    ward_panel, \"Ward\", WARD_TARGET, WARD_FEATURE_COLS\n",
        ")\n",
        "mesh_results_df, mesh_predictions_df, mesh_models, mesh_splits, mesh_features = train_regression_models(\n",
        "    mesh_panel, \"Mesh\", MESH_TARGET, MESH_FEATURE_COLS\n",
        ")\n",
        "\n",
        "ward_results_df.sort_values(\"test_mae\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4dc39893",
      "metadata": {},
      "source": [
        "## SHAP outputs for Streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "6f20a04b",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ignit\\AppData\\Local\\Temp\\ipykernel_4600\\3032209221.py:48: FutureWarning: The NumPy global RNG was seeded by calling `np.random.seed`. In a future version this function will no longer use the global RNG. Pass `rng` explicitly to opt-in to the new behaviour and silence this warning.\n",
            "  shap.summary_plot(shap_values, sample[feature_cols], plot_type=\"bar\", show=False)\n",
            "C:\\Users\\ignit\\AppData\\Local\\Temp\\ipykernel_4600\\3032209221.py:61: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  \"expected_value\": float(expected_value),\n",
            "C:\\Users\\ignit\\AppData\\Local\\Temp\\ipykernel_4600\\3032209221.py:48: FutureWarning: The NumPy global RNG was seeded by calling `np.random.seed`. In a future version this function will no longer use the global RNG. Pass `rng` explicitly to opt-in to the new behaviour and silence this warning.\n",
            "  shap.summary_plot(shap_values, sample[feature_cols], plot_type=\"bar\", show=False)\n"
          ]
        }
      ],
      "source": [
        "# exports shap summaries plus local explanations for tree models.\n",
        "def save_tree_shap_outputs(level_name: str, model_name: str, model, split_df: pd.DataFrame, feature_cols: List[str]):\n",
        "    if split_df.empty:\n",
        "        print(f\"No data for SHAP: {level_name} {model_name}\")\n",
        "        return\n",
        "    try:\n",
        "        explainer = shap.TreeExplainer(model)\n",
        "    except Exception as exc:\n",
        "        print(f\"SHAP explainer failed for {level_name} {model_name}: {exc}\")\n",
        "        return\n",
        "\n",
        "    sample = split_df.sample(n=min(600, len(split_df)), random_state=42)\n",
        "    shap_values = explainer.shap_values(sample[feature_cols])\n",
        "    if isinstance(shap_values, list):\n",
        "        shap_values = shap_values[0]\n",
        "    shap_abs = np.abs(shap_values)\n",
        "    summary_df = pd.DataFrame({\n",
        "        \"Feature\": feature_cols,\n",
        "        \"MeanAbsSHAP\": shap_abs.mean(axis=0),\n",
        "    }).sort_values(\"MeanAbsSHAP\", ascending=False)\n",
        "    summary_path = SHAP_DIR / f\"{level_name.lower()}_{model_name.lower()}_shap_summary.csv\"\n",
        "    summary_df.to_csv(summary_path, index=False)\n",
        "\n",
        "    meta_cols = [\"Ward\", \"Mesh250m\", \"PeriodKey\"]\n",
        "    local_records = []\n",
        "    sample_reset = sample.reset_index(drop=True)\n",
        "    for i in range(len(sample_reset)):\n",
        "        meta = {col: sample_reset.iloc[i].get(col) for col in meta_cols if col in sample_reset.columns}\n",
        "        base = {\n",
        "            \"ObservationIndex\": int(i),\n",
        "            \"Model\": model_name,\n",
        "            \"Level\": level_name,\n",
        "            **meta,\n",
        "        }\n",
        "        for feature_idx, feature in enumerate(feature_cols):\n",
        "            local_records.append({\n",
        "                **base,\n",
        "                \"Feature\": feature,\n",
        "                \"FeatureValue\": float(sample_reset.iloc[i][feature]),\n",
        "                \"SHAPValue\": float(shap_values[i, feature_idx]),\n",
        "            })\n",
        "    local_df = pd.DataFrame(local_records)\n",
        "    local_path = SHAP_DIR / f\"{level_name.lower()}_{model_name.lower()}_shap_local.csv\"\n",
        "    local_df.to_csv(local_path, index=False)\n",
        "\n",
        "    try:\n",
        "        plt.figure(figsize=(8, 4))\n",
        "        shap.summary_plot(shap_values, sample[feature_cols], plot_type=\"bar\", show=False)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(SHAP_DIR / f\"{level_name.lower()}_{model_name.lower()}_shap_bar.png\", dpi=200)\n",
        "        plt.close()\n",
        "    except Exception as exc:\n",
        "        print(f\"SHAP bar plot skipped for {level_name} {model_name}: {exc}\")\n",
        "\n",
        "    metadata_path = SHAP_DIR / f\"{level_name.lower()}_{model_name.lower()}_shap_metadata.json\"\n",
        "    expected_value = explainer.expected_value\n",
        "    if isinstance(expected_value, list):\n",
        "        expected_value = expected_value[0]\n",
        "    with metadata_path.open(\"w\", encoding=\"utf-8\") as fh:\n",
        "        json.dump({\n",
        "            \"expected_value\": float(expected_value),\n",
        "            \"feature_cols\": feature_cols,\n",
        "            \"n_samples\": len(sample_reset),\n",
        "        }, fh, indent=2)\n",
        "\n",
        "\n",
        "for level_name, models, splits, features in [\n",
        "    (\"Ward\", ward_models, ward_splits, ward_features),\n",
        "    (\"Mesh\", mesh_models, mesh_splits, mesh_features),\n",
        "]:\n",
        "    for model_name, model in models.items():\n",
        "        if model_name not in TREE_MODELS:\n",
        "            continue\n",
        "        save_tree_shap_outputs(level_name, model_name, model, splits[\"val\"], features)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "247b24b1",
      "metadata": {},
      "source": [
        "## PyTorch LSTM for ward forecasts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "ee241b3b",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>train_mae</th>\n",
              "      <th>train_rmse</th>\n",
              "      <th>train_r2</th>\n",
              "      <th>val_mae</th>\n",
              "      <th>val_rmse</th>\n",
              "      <th>val_r2</th>\n",
              "      <th>test_mae</th>\n",
              "      <th>test_rmse</th>\n",
              "      <th>test_r2</th>\n",
              "      <th>Level</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>TorchLSTM</td>\n",
              "      <td>26357.412866</td>\n",
              "      <td>37359.440062</td>\n",
              "      <td>0.983402</td>\n",
              "      <td>27351.768309</td>\n",
              "      <td>38194.829837</td>\n",
              "      <td>0.988868</td>\n",
              "      <td>38419.233201</td>\n",
              "      <td>68314.440846</td>\n",
              "      <td>0.975357</td>\n",
              "      <td>Ward</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Model     train_mae    train_rmse  train_r2       val_mae  \\\n",
              "0  TorchLSTM  26357.412866  37359.440062  0.983402  27351.768309   \n",
              "\n",
              "       val_rmse    val_r2      test_mae     test_rmse   test_r2 Level  \n",
              "0  38194.829837  0.988868  38419.233201  68314.440846  0.975357  Ward  "
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# defines sequence helpers and trains the torch lstm.\n",
        "\n",
        "class SequenceDataset(Dataset):\n",
        "    def __init__(self, features: np.ndarray, targets: np.ndarray):\n",
        "        self.x = torch.tensor(features, dtype=torch.float32)\n",
        "        self.y = torch.tensor(targets, dtype=torch.float32).unsqueeze(-1)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.x[idx], self.y[idx]\n",
        "\n",
        "\n",
        "class PriceLSTM(nn.Module):\n",
        "    def __init__(self, input_dim: int, hidden_size: int = 96, dropout: float = 0.3):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output, _ = self.lstm(x)\n",
        "        last_hidden = output[:, -1, :]\n",
        "        last_hidden = self.dropout(last_hidden)\n",
        "        return self.fc(last_hidden)\n",
        "\n",
        "\n",
        "def prepare_sequence_data(panel: pd.DataFrame, feature_cols: List[str], target_col: str, group_col: str = \"Ward\"):\n",
        "    feature_cols = [c for c in feature_cols if c in panel.columns]\n",
        "    df = panel.dropna(subset=feature_cols + [target_col]).copy()\n",
        "    df[\"Order\"] = df[\"PeriodKey\"].apply(period_to_order)\n",
        "    train_mask = df[\"Order\"] <= TRAIN_END_ORDER\n",
        "    means = df.loc[train_mask, feature_cols].mean()\n",
        "    stds = df.loc[train_mask, feature_cols].std().replace(0, 1)\n",
        "    target_raw = df[target_col].astype(np.float32).copy()\n",
        "    df[feature_cols] = (df[feature_cols] - means) / stds\n",
        "    df[\"_target_raw\"] = target_raw\n",
        "\n",
        "    sequence_data = {\n",
        "        \"train\": {\"features\": [], \"targets\": [], \"meta\": []},\n",
        "        \"val\": {\"features\": [], \"targets\": [], \"meta\": []},\n",
        "        \"test\": {\"features\": [], \"targets\": [], \"meta\": []},\n",
        "    }\n",
        "\n",
        "    for key, group in df.groupby(group_col):\n",
        "        group = group.sort_values(\"Order\")\n",
        "        if len(group) <= SEQ_LEN:\n",
        "            continue\n",
        "        feats = group[feature_cols].values.astype(np.float32)\n",
        "        targets = group[\"_target_raw\"].values.astype(np.float32)\n",
        "        for idx in range(SEQ_LEN, len(group)):\n",
        "            window = feats[idx - SEQ_LEN: idx]\n",
        "            y = targets[idx]\n",
        "            order_val = group[\"Order\"].iloc[idx]\n",
        "            if order_val <= TRAIN_END_ORDER:\n",
        "                split = \"train\"\n",
        "            elif order_val <= VAL_END_ORDER:\n",
        "                split = \"val\"\n",
        "            else:\n",
        "                split = \"test\"\n",
        "            meta = {\n",
        "                \"Ward\": key,\n",
        "                \"PeriodKey\": group[\"PeriodKey\"].iloc[idx],\n",
        "                \"WardLat\": group[\"WardLat\"].iloc[idx] if \"WardLat\" in group.columns else np.nan,\n",
        "                \"WardLon\": group[\"WardLon\"].iloc[idx] if \"WardLon\" in group.columns else np.nan,\n",
        "            }\n",
        "            sequence_data[split][\"features\"].append(window)\n",
        "            sequence_data[split][\"targets\"].append(y)\n",
        "            sequence_data[split][\"meta\"].append(meta)\n",
        "\n",
        "    for split in sequence_data:\n",
        "        if sequence_data[split][\"features\"]:\n",
        "            sequence_data[split][\"features\"] = np.stack(sequence_data[split][\"features\"])\n",
        "            sequence_data[split][\"targets\"] = np.array(sequence_data[split][\"targets\"], dtype=np.float32)\n",
        "            sequence_data[split][\"meta\"] = pd.DataFrame(sequence_data[split][\"meta\"])\n",
        "        else:\n",
        "            sequence_data[split][\"features\"] = np.empty((0, SEQ_LEN, len(feature_cols)), dtype=np.float32)\n",
        "            sequence_data[split][\"targets\"] = np.empty((0,), dtype=np.float32)\n",
        "            sequence_data[split][\"meta\"] = pd.DataFrame(columns=[\"Ward\", \"PeriodKey\", \"WardLat\", \"WardLon\"])\n",
        "    scaler_info = {\"mean\": means.to_dict(), \"std\": stds.to_dict(), \"features\": feature_cols}\n",
        "    return sequence_data, scaler_info\n",
        "\n",
        "\n",
        "def _build_loader(split_data):\n",
        "    if split_data[\"features\"].size == 0:\n",
        "        return None\n",
        "    targets = split_data[\"targets\"] / TARGET_SCALE\n",
        "    dataset = SequenceDataset(split_data[\"features\"], targets)\n",
        "    return DataLoader(dataset, batch_size=min(BATCH_SIZE, len(dataset)), shuffle=True)\n",
        "\n",
        "\n",
        "def _evaluate_loader(model, loader):\n",
        "    if loader is None:\n",
        "        return np.nan, np.nan, np.nan\n",
        "    preds, trues = [], []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in loader:\n",
        "            xb = xb.to(DEVICE)\n",
        "            yb = yb.to(DEVICE)\n",
        "            outputs = model(xb)\n",
        "            preds.append(outputs.cpu().numpy())\n",
        "            trues.append(yb.cpu().numpy())\n",
        "    preds = np.vstack(preds).flatten() * TARGET_SCALE\n",
        "    trues = np.vstack(trues).flatten() * TARGET_SCALE\n",
        "    metrics = evaluate_sets(trues, preds)\n",
        "    return metrics[\"mae\"], metrics[\"rmse\"], metrics[\"r2\"]\n",
        "\n",
        "\n",
        "def train_lstm(sequence_data, input_dim: int):\n",
        "    train_loader = _build_loader(sequence_data[\"train\"])\n",
        "    val_loader = _build_loader(sequence_data[\"val\"])\n",
        "    model = PriceLSTM(input_dim=input_dim).to(DEVICE)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "    criterion = nn.L1Loss()\n",
        "    best_state = None\n",
        "    best_val = math.inf\n",
        "    patience_ctr = 0\n",
        "\n",
        "    for _ in range(MAX_EPOCHS):\n",
        "        if train_loader is None:\n",
        "            break\n",
        "        model.train()\n",
        "        for xb, yb in train_loader:\n",
        "            xb = xb.to(DEVICE)\n",
        "            yb = yb.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(xb)\n",
        "            loss = criterion(preds, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        val_mae, _, _ = _evaluate_loader(model, val_loader)\n",
        "        if val_mae < best_val:\n",
        "            best_val = val_mae\n",
        "            patience_ctr = 0\n",
        "            best_state = model.state_dict()\n",
        "        else:\n",
        "            patience_ctr += 1\n",
        "            if patience_ctr >= EARLY_STOPPING_PATIENCE:\n",
        "                break\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "    return model\n",
        "\n",
        "\n",
        "def predict_sequences(model, split_data):\n",
        "    if split_data[\"features\"].size == 0:\n",
        "        return np.array([])\n",
        "    dummy_targets = np.zeros_like(split_data[\"targets\"], dtype=np.float32)\n",
        "    loader = DataLoader(SequenceDataset(split_data[\"features\"], dummy_targets), batch_size=BATCH_SIZE, shuffle=False)\n",
        "    preds = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for xb, _ in loader:\n",
        "            xb = xb.to(DEVICE)\n",
        "            outputs = model(xb)\n",
        "            preds.append(outputs.cpu().numpy())\n",
        "    preds = np.vstack(preds).flatten() * TARGET_SCALE\n",
        "    return preds\n",
        "\n",
        "\n",
        "WARD_LSTM_FEATURES = [\n",
        "    \"MedianPriceSqM\", \"MedianPriceSqM_lag1\", \"MedianPriceSqM_ma4q\",\n",
        "    \"MedianPriceSqM_growth_qoq\", \"TransactionCount\", \"AvgBuildingAge\", \"ActiveMeshes\"\n",
        "]\n",
        "\n",
        "sequence_data, scaler_info = prepare_sequence_data(ward_panel, WARD_LSTM_FEATURES, WARD_TARGET)\n",
        "if sequence_data[\"train\"][\"features\"].size:\n",
        "    lstm_model = train_lstm(sequence_data, input_dim=sequence_data[\"train\"][\"features\"].shape[-1])\n",
        "    lstm_predictions = {}\n",
        "    lstm_results = {}\n",
        "    for split in [\"train\", \"val\", \"test\"]:\n",
        "        preds = predict_sequences(lstm_model, sequence_data[split])\n",
        "        lstm_predictions[split] = preds\n",
        "        if preds.size:\n",
        "            metrics = evaluate_sets(sequence_data[split][\"targets\"], preds)\n",
        "            lstm_results[split] = metrics\n",
        "    ward_lstm_results_df = (\n",
        "        pd.DataFrame({\n",
        "            \"Model\": [\"TorchLSTM\"],\n",
        "            **{f\"{split}_{metric}\": [values.get(metric, np.nan)] for split, values in lstm_results.items() for metric in [\"mae\", \"rmse\", \"r2\"]},\n",
        "        })\n",
        "    )\n",
        "    ward_lstm_results_df[\"Level\"] = \"Ward\"\n",
        "    lstm_prediction_frames = []\n",
        "    for split in [\"train\", \"val\", \"test\"]:\n",
        "        meta = sequence_data[split][\"meta\"].copy()\n",
        "        if meta.empty:\n",
        "            continue\n",
        "        meta[\"Model\"] = \"TorchLSTM\"\n",
        "        meta[\"Level\"] = \"Ward\"\n",
        "        meta[\"Split\"] = split\n",
        "        meta[\"Actual\"] = sequence_data[split][\"targets\"]\n",
        "        meta[\"Predicted\"] = lstm_predictions[split]\n",
        "        lstm_prediction_frames.append(meta)\n",
        "    ward_lstm_predictions_df = pd.concat(lstm_prediction_frames, ignore_index=True)\n",
        "    torch.save(lstm_model.state_dict(), OUTPUT_DIR / \"ward_model_torchlstm.pt\")\n",
        "    with (OUTPUT_DIR / \"ward_model_torchlstm_features.json\").open(\"w\", encoding=\"utf-8\") as fh:\n",
        "        json.dump({\"feature_cols\": scaler_info[\"features\"], **scaler_info}, fh, indent=2)\n",
        "else:\n",
        "    ward_lstm_results_df = pd.DataFrame(columns=[\"Model\", \"train_mae\", \"val_mae\", \"test_mae\"]).assign(Level=\"Ward\")\n",
        "    ward_lstm_predictions_df = pd.DataFrame()\n",
        "    lstm_model = None\n",
        "\n",
        "ward_lstm_results_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ac8aae5",
      "metadata": {},
      "source": [
        "## Export artifacts for Streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "4c81b0f1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Artifacts written:\n",
            "- c:\\Users\\ignit\\OneDrive\\Desktop\\Study\\GeorgiaTech\\CSE6242 - Fall 2025\\Project\\test_notebooks\\model_results.csv\n",
            "- c:\\Users\\ignit\\OneDrive\\Desktop\\Study\\GeorgiaTech\\CSE6242 - Fall 2025\\Project\\test_notebooks\\ward_predictions_detailed.csv\n",
            "- c:\\Users\\ignit\\OneDrive\\Desktop\\Study\\GeorgiaTech\\CSE6242 - Fall 2025\\Project\\test_notebooks\\mesh_predictions_detailed.csv\n",
            "- c:\\Users\\ignit\\OneDrive\\Desktop\\Study\\GeorgiaTech\\CSE6242 - Fall 2025\\Project\\test_notebooks\\model_predictions_viz.csv\n",
            "SHAP files stored under c:\\Users\\ignit\\OneDrive\\Desktop\\Study\\GeorgiaTech\\CSE6242 - Fall 2025\\Project\\test_notebooks\\shap_outputs\n"
          ]
        }
      ],
      "source": [
        "# writes csv artifacts for the streamlit dashboard.\n",
        "all_results = pd.concat(\n",
        "    [\n",
        "        ward_results_df,\n",
        "        mesh_results_df,\n",
        "        ward_lstm_results_df,\n",
        "    ],\n",
        "    ignore_index=True,\n",
        "    sort=False,\n",
        ")\n",
        "all_results.to_csv(OUTPUT_DIR / \"model_results.csv\", index=False)\n",
        "\n",
        "ward_predictions_full = pd.concat(\n",
        "    [\n",
        "        ward_predictions_df,\n",
        "        ward_lstm_predictions_df,\n",
        "    ],\n",
        "    ignore_index=True,\n",
        "    sort=False,\n",
        ")\n",
        "ward_predictions_full.to_csv(OUTPUT_DIR / \"ward_predictions_detailed.csv\", index=False)\n",
        "\n",
        "mesh_predictions_df.to_csv(OUTPUT_DIR / \"mesh_predictions_detailed.csv\", index=False)\n",
        "\n",
        "viz_frames = []\n",
        "if not ward_predictions_full.empty:\n",
        "    ward_viz = ward_predictions_full.copy()\n",
        "    ward_viz[\"Latitude\"] = ward_viz[\"WardLat\"]\n",
        "    ward_viz[\"Longitude\"] = ward_viz[\"WardLon\"]\n",
        "    viz_frames.append(ward_viz)\n",
        "if not mesh_predictions_df.empty:\n",
        "    mesh_viz = mesh_predictions_df.copy()\n",
        "    mesh_viz[\"Latitude\"] = mesh_viz[\"MeshLat\"].fillna(mesh_viz[\"WardLat\"])\n",
        "    mesh_viz[\"Longitude\"] = mesh_viz[\"MeshLon\"].fillna(mesh_viz[\"WardLon\"])\n",
        "    viz_frames.append(mesh_viz)\n",
        "\n",
        "if viz_frames:\n",
        "    model_predictions_viz = pd.concat(viz_frames, ignore_index=True, sort=False)\n",
        "    model_predictions_viz.to_csv(OUTPUT_DIR / \"model_predictions_viz.csv\", index=False)\n",
        "\n",
        "print(\"Artifacts written:\")\n",
        "for path in [\n",
        "    OUTPUT_DIR / \"model_results.csv\",\n",
        "    OUTPUT_DIR / \"ward_predictions_detailed.csv\",\n",
        "    OUTPUT_DIR / \"mesh_predictions_detailed.csv\",\n",
        "    OUTPUT_DIR / \"model_predictions_viz.csv\",\n",
        "]:\n",
        "    if path.exists():\n",
        "        print(f\"- {path}\")\n",
        "print(f\"SHAP files stored under {SHAP_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29fd07ce",
      "metadata": {},
      "source": [
        "### How to run\n",
        "1. Execute every cell sequentially after refreshing `main_features` / `mesh_quarter_features`.\n",
        "2. Point the Streamlit app to the CSVs in `test_notebooks/` and the SHAP files under `test_notebooks/shap_outputs/`.\n",
        "3. (Optional) load `ward_model_torchlstm.pt` plus the JSON feature config if you want to reuse the LSTM weights elsewhere."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
