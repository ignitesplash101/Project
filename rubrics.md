Progress Report
No more than 4 letter-size pages (excluding references), 11pt font, typed. Use at least 1-inch margin for each page (top, right, bottom, left).

It mainly serves as a checkpoint, to detect and prevent dead-ends and other problems early on.

Organize the report using the same sections as your final report (1. Introduction; 2. Problem Definition; 3. Literature Survey; etc), with a few sections "under construction", describing the work performed up to then, and the revised plans for the whole project.

Specifically, the introduction and literature survey sections should be in their final form. The section on the proposed method should be almost finished. The sections about experiments and conclusions will have whatever results you have obtained, as well as “place-holders” for the results you plan/hope to obtain.

Grading scheme & Submission instructions
[70%] Detailed description of your team’s methods (should be almost finished)
Provide a clear list of innovations: give a list of the best 2-4 ideas that your approach exhibits.
This section should be almost finished. That is, we expect your team to have made substantial progress in the design and implementation of BOTH the computation and visualization components.
[25%] Detailed description of the design of upcoming experiments for evaluating your approaches
[5%] Plan of activities (same requirements as described in “Proposal” above)
Provide the old plan and the revised plan
[-5% if not included] Provide a statement that summarizes the distribution of team members’ effort. The summary statement can be as simple as "all team members have contributed a similar amount of effort". Place this statement immediately after the Gantt chart (or table). If effort distribution is too uneven, we may assign higher scores to members who have contributed more.
Include your team project’s title, team number, and all team member names (we recommend at the top of the first page)
Team's contact person submits a softcopy (progress report only), named teamXXXprogress.pdf, where XXX is the team number (e.g., team001progress.pdf for team 1). Visit Canvas for submission instructions.


An appendix is for optional, non-essential information. We may not read or even grade it.


Do we need to explicitly answer all Heilmeier questions in the Progress Report?
You are not required to explicitly answer the Heilmeier questions in the Progress Report like you did in the proposal. The main purpose of the Heilmeier questions (at the proposal stage) was to help your team think through relevant parts of the problem being studied and the proposed solutions, to figure out a plan of what to do. As you likely have already figured out, some content from the proposal is still very much relevant (e.g., the main ideas and possible technical approaches) and you team may likely want to expand on such content. And some content may need to be modified or even removed.

The progress report may be written based on your proposal. For example, the literature survey in the progress report is not required to be identical to the literature survey in the proposal. That is, you may update the proposal's literature survey as needed. Of course, the number of papers should not drop below the requirement (3 papers/team member), and the quality of discussion should still be equal or better than that in the proposal.



What do you mean by “evaluation”?
To evaluate your team’s proposed approaches, you come up with experiments to test how well they address the problems your team wants to solve. In general, when you propose a new idea, you need to show that your idea works well --- experiments is one way to demonstrate this. Ultimately, you will want to deploy your approach and have people use them, which will be the definitive way to measure your approach's "success" but that clearly will take time, thus is not required in this class project.


The type of experiments depend on your proposed approach(es) and problem(s). Suppose a team is proposing an movie recommendation tool that allows users to interactively specify movies that they like and receive recommendations in real time, then possible experiments may include:


1) Scalability evaluation, e.g., how well the recommendation algorithm scales with data (e.g., wall clock time v.s. #movies in dataset)


2) Recommendation accuracy, e.g., using MovieLens datasets


3) Usability evaluation of the tool, e.g., via user study (for a class project, it's acceptable to invite classmates; for research that you plan to publish, you will need to go through the formal IRB protocol process)

